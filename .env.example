# =============================================================================
# OpenHands GPU Setup - Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values
# =============================================================================

# -----------------------------------------------------------------------------
# LLM Provider Configuration (Choose one)
# -----------------------------------------------------------------------------

# ★★★ RECOMMENDED: DeepSeek Local (GPU accelerated via Ollama) ★★★
LLM_MODEL=ollama/deepseek-coder-v2:16b
LLM_API_KEY=ollama
LLM_BASE_URL=http://host.docker.internal:11434

# Option 2: DeepSeek API (Cloud)
# LLM_MODEL=deepseek/deepseek-chat
# LLM_API_KEY=your-deepseek-api-key
# LLM_BASE_URL=https://api.deepseek.com/v1

# Option 3: OpenAI
# LLM_MODEL=gpt-4o
# LLM_API_KEY=your-openai-api-key-here
# LLM_BASE_URL=https://api.openai.com/v1

# Option 4: Anthropic Claude
# LLM_MODEL=claude-sonnet-4-20250514
# LLM_API_KEY=your-anthropic-api-key-here
# LLM_BASE_URL=https://api.anthropic.com

# Option 5: Local Ollama with other models (GPU accelerated)
# LLM_MODEL=ollama/llama3.1:70b
# LLM_API_KEY=ollama
# LLM_BASE_URL=http://host.docker.internal:11434

# Option 6: Local LM Studio
# LLM_MODEL=lmstudio/your-model-name
# LLM_API_KEY=lm-studio
# LLM_BASE_URL=http://host.docker.internal:1234/v1

# Option 7: Azure OpenAI
# LLM_MODEL=azure/your-deployment-name
# LLM_API_KEY=your-azure-api-key
# LLM_BASE_URL=https://your-resource.openai.azure.com

# Option 8: Google Gemini
# LLM_MODEL=gemini/gemini-pro
# LLM_API_KEY=your-google-api-key
# LLM_BASE_URL=https://generativelanguage.googleapis.com

# -----------------------------------------------------------------------------
# OpenHands Configuration
# -----------------------------------------------------------------------------

# Port for OpenHands web interface
OPENHANDS_PORT=3000

# Workspace directory (where your projects will be stored)
WORKSPACE_DIR=./workspace

# OpenHands state directory (for persistence)
OPENHANDS_STATE_DIR=./openhands-state

# -----------------------------------------------------------------------------
# Ollama Configuration (if using local LLM)
# -----------------------------------------------------------------------------

# Port for Ollama API
OLLAMA_PORT=11434

# Directory to store Ollama models
OLLAMA_MODELS_DIR=./ollama-models

# -----------------------------------------------------------------------------
# LocalAI Configuration (alternative local LLM)
# -----------------------------------------------------------------------------

# Port for LocalAI API
LOCALAI_PORT=8080

# Directory to store LocalAI models
LOCALAI_MODELS_DIR=./localai-models

# Number of threads for LocalAI
LOCALAI_THREADS=4

# -----------------------------------------------------------------------------
# GPU Configuration
# -----------------------------------------------------------------------------

# NVIDIA GPU settings (usually auto-detected)
# NVIDIA_VISIBLE_DEVICES=all
# NVIDIA_DRIVER_CAPABILITIES=compute,utility

# -----------------------------------------------------------------------------
# Advanced Configuration
# -----------------------------------------------------------------------------

# Sandbox runtime image
SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:main

# Enable debug logging
# LOG_ALL_EVENTS=true
